Device: CUDA
Number of GPUs: 1
--------------------------------------------------------------------------------
+----+-----------------------------+--------------------------------------------------+
|    | keys                        | values                                           |
|----+-----------------------------+--------------------------------------------------|
|  0 | epochs                      | 2                                                |
|  1 | cuda                        | True                                             |
|  2 | batch_size                  | 64                                               |
|  3 | lr                          | 0.0001                                           |
|  4 | seed                        |                                                  |
|  5 | patience                    | 5                                                |
|  6 | model                       | roberta-base                                     |
|  7 | tokenizer                   | roberta-base                                     |
|  8 | subset                      | 1.0                                              |
|  9 | use_adapter                 | True                                             |
| 10 | shuffle_rate                |                                                  |
| 11 | cache_token_encodings       | False                                            |
| 12 | non_sequential              | True                                             |
| 13 | adapter_names               | entity_predict--amp                              |
| 14 | input_dir                   | /home/simon/wikidata5m                           |
| 15 | output_dir                  | checkpoints                                      |
| 16 | trained_model               |                                                  |
| 17 | amp                         | False                                            |
| 18 | save_step                   | 2000                                             |
| 19 | max_seq_length              | 64                                               |
| 20 | num_workers                 | 32                                               |
| 21 | warmup_proportion           | 0.1                                              |
| 22 | CRate                       | 8                                                |
| 23 | n_partition                 | 10                                               |
| 24 | sub_group_idx               |                                                  |
| 25 | bi_direction                | False                                            |
| 26 | adapter_layers              |                                                  |
| 27 | gradient_accumulation_steps | 1                                                |
| 28 | model_str                   | roberta-base_20220127_130456_adapter             |
| 29 | save_path                   | checkpoints/roberta-base_20220127_130456_adapter |
+----+-----------------------------+--------------------------------------------------+
--------------------------------------------------------------------------------
generate random seed 1643259896
Loading entities (subset mode:1.0) ent_total:4813491 len(self.id2ent): 4813491
Read Relation File
825 relations loaded.
['instance of', 'languages spoken, written or signed', 'director', 'country of citizenship', 'member of sports team', 'located in the administrative territorial entity', 'place of birth', 'followed by', 'cast member', 'exhibition history']
Initializing model from roberta-base
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Get 5000 examples of Node Prediction With Partition datasets from partition instance of 0/10 set
Execute [_create_examples] method costing 7.18 ms
Start Training on group_idx 0
Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]








Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  4.92it/s]
Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.48701882362366
Execute [train_epoch] method costing 18931.54 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_0_epoch_0/








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.16s/it]380.6195824146271
Execute [train_epoch] method costing 18262.50 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_0_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.83s/it]
Initializing model from roberta-base
Get 5000 examples of Node Prediction With Partition datasets from partition languages spoken, written or signed 1/10 set
Execute [_create_examples] method costing 15.77 ms
Start Training on group_idx 1
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.09it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.









Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.1424126625061
Execute [train_epoch] method costing 19440.12 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_1_epoch_0/
Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.63s/it]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.63s/it]380.5804419517517
Execute [train_epoch] method costing 19440.49 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_1_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.63s/it]

Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  4.83it/s]
Get 5000 examples of Node Prediction With Partition datasets from partition director 2/10 set
Execute [_create_examples] method costing 15.60 ms
Start Training on group_idx 2
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  4.83it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.03915309906006
Execute [train_epoch] method costing 18500.49 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_2_epoch_0/
Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:18<00:18, 18.71s/it]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:18<00:18, 18.71s/it]380.61624479293823
Execute [train_epoch] method costing 18894.58 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_2_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.91s/it]

Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  5.10it/s]
Get 5000 examples of Node Prediction With Partition datasets from partition country of citizenship 3/10 set
Execute [_create_examples] method costing 19.13 ms
Start Training on group_idx 3
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.10it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.39081144332886
Execute [train_epoch] method costing 18589.15 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_3_epoch_0/

Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  4.95it/s]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:18<00:18, 18.80s/it]380.62997102737427
Execute [train_epoch] method costing 18476.23 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_3_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.75s/it]
Initializing model from roberta-base
Get 5000 examples of Node Prediction With Partition datasets from partition member of sports team 4/10 set
Execute [_create_examples] method costing 14.34 ms
Start Training on group_idx 4
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.03it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]361.34505891799927
Execute [train_epoch] method costing 18604.16 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_4_epoch_0/
Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  5.05it/s]









Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:18<00:18, 18.84s/it]332.8853476047516
Execute [train_epoch] method costing 18726.19 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_4_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.90s/it]
Initializing model from roberta-base
Get 5000 examples of Node Prediction With Partition datasets from partition located in the administrative territorial entity 5/10 set
Execute [_create_examples] method costing 67.75 ms
Start Training on group_idx 5
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.03it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.51s/it]381.388578414917
Execute [train_epoch] method costing 19309.01 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_5_epoch_0/
Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  4.84it/s]









Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.66s/it]368.18420815467834
Execute [train_epoch] method costing 19607.06 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_5_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.66s/it]368.18420815467834

Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:19<00:00,  4.85it/s]
Get 5000 examples of Node Prediction With Partition datasets from partition place of birth 6/10 set
Execute [_create_examples] method costing 14.83 ms
Start Training on group_idx 6
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  4.85it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.1235785484314
Execute [train_epoch] method costing 18923.58 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_6_epoch_0/
Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  5.02it/s]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.13s/it]380.57322120666504
Execute [train_epoch] method costing 18818.31 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_6_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:38<00:00, 19.08s/it]
Initializing model from roberta-base
Get 5000 examples of Node Prediction With Partition datasets from partition followed by 7/10 set
Execute [_create_examples] method costing 15.94 ms
Start Training on group_idx 7
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.01it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.0902752876282
Execute [train_epoch] method costing 18694.21 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_7_epoch_0/
Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  5.04it/s]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:18<00:18, 18.93s/it]380.5597414970398
Execute [train_epoch] method costing 18755.08 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_7_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.96s/it]
Initializing model from roberta-base
Get 5000 examples of Node Prediction With Partition datasets from partition cast member 8/10 set
Execute [_create_examples] method costing 83.20 ms
Start Training on group_idx 8
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  5.05it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.








Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]381.2413191795349
Execute [train_epoch] method costing 18980.49 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_8_epoch_0/
Training:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 78/79 [00:18<00:00,  4.95it/s]








Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:19<00:19, 19.18s/it]369.7438282966614
Execute [train_epoch] method costing 19047.52 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_8_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:38<00:00, 19.21s/it]
Initializing model from roberta-base
Get 479 examples of Node Prediction With Partition datasets from partition exhibition history 9/10 set
Execute [_create_examples] method costing 1.33 ms
Start Training on group_idx 9
Some weights of RelPrompt were not initialized from the model checkpoint at roberta-base and are newly initialized: ['prompt', 'lm_head.decoder.bias']0:00,  4.88it/s]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]38.575870990753174
Execute [train_epoch] method costing 4671.38 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_9_epoch_0/
Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  4.17it/s]

Epoch:  50%|██████████████████████████████████████████████████████████████                                                              | 1/2 [00:04<00:04,  4.91s/it]38.048311710357666
Execute [train_epoch] method costing 4639.00 ms
saving model to checkpoints/roberta-base_20220127_130456_adapter/group_9_epoch_1/
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.89s/it]